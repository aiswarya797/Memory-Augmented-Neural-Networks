{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MANN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA8O/XX8ANkQo7NmGeR/2X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiswarya797/Memory-Augmented-Neural-Networks/blob/master/MANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cs9svTPv1I",
        "colab_type": "text"
      },
      "source": [
        "### **References**\n",
        "\n",
        "* https://github.com/llSourcell/How-to-Learn-from-Little-Data/blob/master/MANN/Model.py\n",
        "\n",
        "* https://www.youtube.com/watch?v=Ry1IIjLnumI\n",
        "\n",
        "* https://www.slideshare.net/KatyLee4/meta-learning-with-memory-augmented-neural-network\n",
        "\n",
        "* https://github.com/MohamedAfham/Omniglot-Dataset-Classification-using-Memory-Augmented-Neural-Networks\n",
        "\n",
        "* https://science.sciencemag.org/content/350/6266/1332\n",
        "\n",
        "* https://cs330.stanford.edu/\n",
        "\n",
        "* https://www.google.com/search?client=ubuntu&hs=afV&channel=fs&sxsrf=ALeKk02UDnHMDTndE6KR00PVp_oZ98f3UA%3A1592378030176&ei=rsLpXtesCpDB3LUPnvqP2AM&q=Memory+augmented+neural+network+for+omniglot&oq=Memory+augmented+neural+network+for+omniglot&gs_lcp=CgZwc3ktYWIQAzIHCCEQChCgAToECAAQRzoECCMQJzoGCAAQFhAeOggIIRAWEB0QHjoFCCEQoAE6BAghEBVQh8MjWMncI2Dq3SNoAHABeACAAZcCiAHBF5IBBDItMTKYAQCgAQGqAQdnd3Mtd2l6&sclient=psy-ab&ved=0ahUKEwjXu9i2pojqAhWQILcAHR79AzsQ4dUDCAs&uact=5\n",
        "\n",
        "* https://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV26I0mKh62M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWROXgaCiLI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "5ab24bb8-afa0-4751-ea36-03aa8fdad767"
      },
      "source": [
        "# 2. Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdIht1OJYmSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "020631a9-1eca-4625-b7c7-2fe5b7916131"
      },
      "source": [
        "#https://github.com/vineetjain96/one-shot-mann/blob/master/mann/utils/images.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "from scipy.ndimage import rotate,shift\n",
        "# from scipy.misc import imread,imresize\n",
        "from imageio import imread\n",
        "from numpy import resize as imresize\n",
        "\n",
        "# import scipy.misc.imresize as imresize\n",
        "# import scipy.imageio.imread as imread\n",
        "\n",
        "def get_sampled_data(character_folders, nb_classes=5, nb_samples=10*5):\n",
        "\tsampled_characters = random.sample(character_folders, nb_classes)\n",
        "\tlabels_and_images = [(label, os.path.join(character, image_path)) for label, character in zip(np.arange(nb_classes), sampled_characters) for image_path in os.listdir(character)]\n",
        "\tsampled_data = random.sample(labels_and_images, nb_samples)\n",
        "\treturn sampled_data\n",
        "\n",
        "\n",
        "def transform_image(image_path, angle=0., s=(0,0), size=(20,20)):\n",
        "    original = imread(image_path, as_gray=True)#.flatten()\t#, flatten=True)\n",
        "    rotated = np.maximum(np.minimum(rotate(original, angle=angle, cval=1.), 1.), 0.)\n",
        "    shifted = shift(rotated, shift=s)\n",
        "    resized = np.asarray(rotated.resize(size), dtype=np.float32)/255\n",
        "    inverted = 1. - resized\n",
        "    max_value = np.max(inverted)\n",
        "    if max_value > 0:\n",
        "        inverted /= max_value\n",
        "    return inverted\n",
        "\n",
        "# cosine similarity for content based addressing\n",
        "def cosine_similarity(x, y, eps=1e-6):\n",
        "  z = tf.matmul(x, tf.transpose(y, perm=[0,2,1])) \n",
        "  z /= tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(x), 2, keep_dims=True), tf.reduce_sum(tf.square(x), 2, keep_dims=True) + eps))\n",
        "  return z\n",
        "\n",
        "def variable_float32(x, name=''):\n",
        "  return tf.Variable(tf.cast(np.asarray(x, dtype=np.float32), dtype=tf.float32), name=name)\n",
        "\n",
        "def variable_one_hot(shape, name=''):\n",
        "  initial = np.zeros(shape, dtype=np.float32)\n",
        "  initial[...,0] = 1\n",
        "  return tf.Variable(tf.cast(initial, dtype=tf.float32), name=name)\n",
        "\n",
        "\n",
        "class OmniglotGenerator(object):\n",
        "\n",
        "\tdef __init__(self, data_folder, batch_size=1, nb_classes=5, nb_samples=10*5, max_rotation=np.pi/6, \\\n",
        "\t\t\tmax_shift=10, img_size=(20, 20)):\n",
        "\t\tself.data_folder = data_folder\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.nb_classes = nb_classes\n",
        "\t\tself.nb_samples = nb_samples\n",
        "\t\tself.max_rotation = max_rotation\n",
        "\t\tself.max_shift = max_shift\n",
        "\t\tself.img_size = img_size\n",
        "\t\tself.character_folders = [os.path.join(data_folder, alphabet, character) for alphabet in os.listdir(data_folder) \\\n",
        "         \t\t\t\t\t\tif os.path.isdir(os.path.join(data_folder, alphabet)) \\\n",
        "         \t\t\t\t\t\tfor character in os.listdir(os.path.join(data_folder, alphabet))]\n",
        "\t\n",
        "\tdef episode(self):\n",
        "\t\tepisode_input = np.zeros((self.batch_size, self.nb_samples, np.prod(self.img_size)), dtype=np.float32)\n",
        "\t\tepisode_output = np.zeros((self.batch_size, self.nb_samples), dtype=np.int32)\n",
        "\n",
        "\t\tfor i in range(self.batch_size):\n",
        "\t\t\tsampled_data = get_sampled_data(self.character_folders, nb_classes=self.nb_classes, nb_samples=self.nb_samples)\n",
        "\t\t\tsequence_length = len(sampled_data)\n",
        "\t\t\tlabels, image_files = zip(*sampled_data)\n",
        "\n",
        "\t\t\tangles = np.random.uniform(-self.max_rotation, self.max_rotation, size=sequence_length)\n",
        "\t\t\tshifts = np.random.randint(-self.max_shift, self.max_shift + 1, size=(sequence_length, 2))\n",
        "\n",
        "\t\t\tepisode_input[i] = np.asarray([transform_image(filename, angle=angle, s=shift, size=self.img_size).flatten() \\\n",
        "\t        \t \t\t\t\tfor (filename, angle, shift) in zip(image_files, angles, shifts)], dtype=np.float32)\n",
        "\t\t\tepisode_output[i] = np.asarray(labels, dtype=np.int32)\n",
        "\n",
        "\t\treturn episode_input, episode_output"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VUov9rYPKHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class mann(object):\n",
        "\n",
        "\tdef __init__(self, input_size=20*20, memory_size=(128, 40),controller_size=200, nb_reads=4, nb_classes=5, batch_size=16):\n",
        "\t\tself.input_size = input_size\n",
        "\t\tself.memory_size = memory_size\n",
        "\t\tself.controller_size = controller_size\n",
        "\t\tself.nb_reads = nb_reads\n",
        "\t\tself.nb_classes = nb_classes\n",
        "\t\tself.batch_size = batch_size\n",
        "\n",
        "\tdef initialize(self):\n",
        "\t\t# controller is an LSTM\n",
        "\t\tM_0 = variable_float32(1e-6 * np.ones((self.batch_size,) + self.memory_size), name='memory')\n",
        "\t\tc_0 = variable_float32(np.zeros((self.batch_size, self.controller_size)), name='controller_cell_state')\n",
        "\t\th_0 = variable_float32(np.zeros((self.batch_size, self.controller_size)), name='controller_hidden_state')\n",
        "\t\tr_0 = variable_float32(np.zeros((self.batch_size, self.nb_reads * self.memory_size[1])), name='read_vector')\n",
        "\t\twr_0 = variable_one_hot((self.batch_size, self.nb_reads, self.memory_size[0]), name='wr')\n",
        "\t\twu_0 = variable_one_hot((self.batch_size, self.memory_size[0]), name='wu')\n",
        "\n",
        "\t\treturn [M_0, c_0, h_0, r_0, wr_0, wu_0]\n",
        "\n",
        "\tdef step(self, initializer, x_t):\n",
        "\t\tM_tm1, c_tm1, h_tm1, r_tm1, wr_tm1, wu_tm1 = initializer\n",
        "\t\t\n",
        "\t\twith tf.variable_scope('weights0', reuse=True):\n",
        "\t\t\tW_key = tf.get_variable('W_key', shape=(self.nb_reads, self.controller_size, self.memory_size[1]))\n",
        "\t\t\tb_key = tf.get_variable('b_key', shape=(self.nb_reads, self.memory_size[1]))\n",
        "\n",
        "\t\t\tW_sigma = tf.get_variable('W_sigma', shape=(self.nb_reads, self.controller_size, 1))\n",
        "\t\t\tb_sigma = tf.get_variable('b_sigma', shape=(self.nb_reads, 1))\n",
        "\n",
        "\t\t\tW_xh = tf.get_variable('W_xh', shape=(self.input_size + self.nb_classes, 4*self.controller_size))\n",
        "\t\t\tW_hh = tf.get_variable('W_hh', shape=(self.controller_size, 4*self.controller_size))\n",
        "\t\t\tb_h = tf.get_variable('b_h', shape=(4*self.controller_size))\n",
        "\n",
        "\t\t\tW_o = tf.get_variable('W_o', shape=(self.controller_size + self.nb_reads * self.memory_size[1], self.nb_classes))\n",
        "\t\t\tb_o = tf.get_variable('b_o', shape=(self.nb_classes))\n",
        "\n",
        "\t\t\tgamma = 0.95\n",
        "\n",
        "\t\tdef lstm_step(size, x_t, c_tm1, h_tm1, W_xh, W_hh, b_h):\n",
        "\n",
        "\t\t\tpreactivations = tf.matmul(x_t, W_xh) + tf.matmul(h_tm1, W_hh) + b_h\n",
        "\n",
        "\t\t\tgf = tf.sigmoid(preactivations[:, 0:size])\n",
        "\t\t\tgi = tf.sigmoid(preactivations[:, size:2*size])\n",
        "\t\t\tgo = tf.sigmoid(preactivations[:, 2*size:3*size])\n",
        "\t\t\tu = tf.tanh(preactivations[:, 3*size:4*size])\n",
        "\n",
        "\t\t\tc_t = gf*c_tm1 + gi*u\n",
        "\t\t\th_t = go*tf.tanh(c_t)\n",
        "\n",
        "\t\t\treturn [c_t, h_t]\n",
        "\n",
        "\t\t[c_t, h_t] = lstm_step(self.controller_size, x_t, c_tm1, h_tm1, W_xh, W_hh, b_h)\n",
        "\n",
        "\t\tshape_key = (self.batch_size, self.nb_reads, self.memory_size[1])\n",
        "\t\tshape_sigma = (self.batch_size, self.nb_reads, 1)\n",
        "\n",
        "\t\t_W_key = tf.reshape(W_key, shape=(self.controller_size, -1))\n",
        "\t\t_W_sigma = tf.reshape(W_sigma, shape=(self.controller_size, -1))\n",
        "\n",
        "\t\tk_t = tf.tanh(tf.reshape(tf.matmul(h_t, _W_key), shape=shape_key) + b_key)\n",
        "\t\tsigma_t = tf.sigmoid(tf.reshape(tf.matmul(h_t, _W_sigma), shape=shape_sigma) + b_sigma)\n",
        "\n",
        "\t\t_, indices = tf.nn.top_k(wu_tm1, k=self.memory_size[0])\n",
        "\t\twlu_tm1 = tf.slice(indices, [0,self.memory_size[0] - self.nb_reads], [self.batch_size,self.nb_reads])\n",
        "\t\twlu_tm1 = tf.cast(wlu_tm1, dtype=tf.int32)\n",
        "\t\trow_idx = tf.reshape(tf.tile(tf.reshape(wlu_tm1[:,0], shape=(-1, 1)), (1, self.memory_size[1])), [-1])\n",
        "\t\trow_idx += self.memory_size[0] * tf.reshape(tf.tile(tf.reshape(list(range(self.batch_size)), shape=(-1, 1)), (1, self.memory_size[1])), [-1])\n",
        "\t\t\n",
        "\t\tcol_idx = tf.tile(list(range(self.memory_size[1])), [self.batch_size])\n",
        "\t\t\n",
        "\t\tcoords = tf.transpose(tf.stack([row_idx, col_idx]))\n",
        "\t\tbinary_mask = tf.cast(tf.sparse_to_dense(coords, (self.batch_size*self.memory_size[0], self.memory_size[1]), 1), tf.bool)\n",
        "\t\t\n",
        "\t\tM_t = tf.where(binary_mask, tf.constant(0., shape=(self.batch_size*self.memory_size[0], self.memory_size[1])), tf.reshape(M_tm1, shape=(self.batch_size*self.memory_size[0], self.memory_size[1])))\n",
        "\t\tM_t = tf.reshape(M_t, shape=(self.batch_size, self.memory_size[0], self.memory_size[1]))\n",
        "\n",
        "\t\twlu_tm1 = tf.one_hot(wlu_tm1, self.memory_size[0], axis=-1)\n",
        "\t\tww_t = tf.multiply(sigma_t, wr_tm1) + tf.multiply(1.-sigma_t, wlu_tm1)\n",
        "\n",
        "\t\tM_t = M_t + tf.matmul(tf.transpose(ww_t, perm=[0,2,1]), k_t)\n",
        "\t\t\n",
        "\t\tK_t = cosine_similarity(k_t, M_t)\n",
        "\t\twr_t = tf.nn.softmax(K_t)\n",
        "\n",
        "\t\twu_t = gamma*wu_tm1 + tf.reduce_sum(wr_t, axis=1)+ tf.reduce_sum(ww_t, axis=1)\n",
        "\t\tr_t = tf.reshape(tf.matmul(wr_t, M_t), shape=(self.batch_size,-1))\n",
        "\n",
        "\t\treturn [M_t, c_t, h_t, r_t, wr_t, wu_t]\n",
        "\n",
        "\tdef compute_output(self, input_var, target_var):\n",
        "\t\tM_0, c_0, h_0, r_0, wr_0, wu_0 = self.initialize()\n",
        "\t\t\n",
        "\n",
        "\t\twith tf.variable_scope('weights0'):\n",
        "\t\t\tW_key = tf.get_variable('W_key', shape=(self.nb_reads, self.controller_size, self.memory_size[1]))\n",
        "\t\t\tb_key = tf.get_variable('b_key', shape=(self.nb_reads, self.memory_size[1]))\n",
        "\n",
        "\t\t\tW_sigma = tf.get_variable('W_sigma', shape=(self.nb_reads, self.controller_size, 1))\n",
        "\t\t\tb_sigma = tf.get_variable('b_sigma', shape=(self.nb_reads, 1))\n",
        "\n",
        "\t\t\tW_xh = tf.get_variable('W_xh', shape=(self.input_size + self.nb_classes, 4*self.controller_size))\n",
        "\t\t\tW_hh = tf.get_variable('W_hh', shape=(self.controller_size, 4*self.controller_size))\n",
        "\t\t\tb_h = tf.get_variable('b_h', shape=(4*self.controller_size))\n",
        "\n",
        "\t\t\tW_o = tf.get_variable('W_o', shape=(self.controller_size + self.nb_reads * self.memory_size[1], self.nb_classes))\n",
        "\t\t\tb_o = tf.get_variable('b_o', shape=(self.nb_classes))\n",
        "\n",
        "\t\t\tgamma = 0.95\n",
        "\n",
        "\t\tsequence_length = input_var.get_shape().as_list()[1]\n",
        "\n",
        "\t\tone_hot_target = tf.one_hot(target_var, self.nb_classes, axis=-1)\n",
        "\t\toffset_target_var = tf.concat([tf.zeros_like(tf.expand_dims(one_hot_target[:,0], 1)), one_hot_target[:,:-1]], axis=1)\n",
        "\t\tntm_input = tf.concat([input_var, offset_target_var], axis=2)\n",
        "\n",
        "\t\telems = tf.transpose(ntm_input, perm=[1,0,2])\n",
        "\t\t\n",
        "\t\t#ntm_var = tf.scan(self.step,elems,M_0, c_0, h_0, r_0, wr_0, wu_0)\n",
        "\t\tntm_var = tf.scan(self.step, elems= elems, initializer=[M_0, c_0, h_0, r_0, wr_0, wu_0])\n",
        "\t\tntm_output = tf.transpose(tf.concat(ntm_var[2:4], axis=2), perm=[1,0,2])\n",
        "\n",
        "\t\tprint('Done')\n",
        "\t\tprint()\n",
        "\t\tprint()\n",
        "\t\tepsilon = 0.0001\n",
        "\t\toutput_var = tf.matmul(tf.reshape(ntm_output, shape=(self.batch_size*sequence_length, -1)), W_o) + b_o\n",
        "\t\toutput_var = tf.reshape(output_var, shape=(self.batch_size, sequence_length, -1))\n",
        "\t\toutput_var = tf.nn.softmax(output_var)+epsilon\n",
        "\n",
        "\t\tparams = [W_key, b_key, W_sigma, b_sigma, W_xh, W_hh, b_h, W_o, b_o]\n",
        "\n",
        "\t\treturn output_var, params"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-VGMoFQwB2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fa57b1d0-9ec6-41d5-d012-941870d7a8a4"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NB_CLASSES = 5\n",
        "NB_SAMPLES = 10*5\n",
        "INPUT_HEIGHT = 20\n",
        "INPUT_WIDTH = 20\n",
        "\n",
        "NB_READS = 4\n",
        "CONTROLLER_SIZE = 200\n",
        "MEMORY_LOCATIONS = 128\n",
        "MEMORY_WORD_SIZE = 40\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "ITERATIONS = 100000\n",
        "def build_argparser():\n",
        "\tparser = ArgumentParser()\n",
        "\tBATCH_SIZE = 16\n",
        "\tNB_CLASSES = 5\n",
        "\tNB_SAMPLES = 10*5\n",
        "\tINPUT_HEIGHT = 20\n",
        "\tINPUT_WIDTH = 20\n",
        "\n",
        "\tNB_READS = 4\n",
        "\tCONTROLLER_SIZE = 200\n",
        "\tMEMORY_LOCATIONS = 128\n",
        "\tMEMORY_WORD_SIZE = 40\n",
        "\n",
        "\tLEARNING_RATE = 1e-4\n",
        "\tITERATIONS = 100000\n",
        "\n",
        "\tparser.add_argument('--batch-size',\n",
        "\t\t\tdest='_batch_size',\thelp='Batch size (default: %(default)s)',\n",
        "\t\t\ttype=int, default=BATCH_SIZE)\n",
        "\tparser.add_argument('--num-classes',\n",
        "\t\t\tdest='_nb_classes', help='Number of classes in each episode (default: %(default)s)',\n",
        "\t\t\ttype=int, default=NB_CLASSES)\n",
        "\tparser.add_argument('--num-samples',\n",
        "\t\t\tdest='_nb_samples', help='Number of total samples in each episode (default: %(default)s)',\n",
        "\t\t\ttype=int, default=NB_SAMPLES)\n",
        "\tparser.add_argument('--input-height',\n",
        "\t\t\tdest='_input_height', help='Input image height (default: %(default)s)',\n",
        "\t\t\ttype=int, default=INPUT_HEIGHT)\n",
        "\tparser.add_argument('--input-width',\n",
        "\t\t\tdest='_input_width', help='Input image width (default: %(default)s)',\n",
        "\t\t\ttype=int, default=INPUT_WIDTH)\n",
        "\tparser.add_argument('--num-reads',\n",
        "\t\t\tdest='_nb_reads', help='Number of read heads (default: %(default)s)',\n",
        "\t\t\ttype=int, default=NB_READS)\n",
        "\tparser.add_argument('--controller-size',\n",
        "\t\t\tdest='_controller_size', help='Number of hidden units in controller (default: %(default)s)',\n",
        "\t\t\ttype=int, default=CONTROLLER_SIZE)\n",
        "\tparser.add_argument('--memory-locations',\n",
        "\t\t\tdest='_memory_locations', help='Number of locations in the memory (default: %(default)s)',\n",
        "\t\t\ttype=int, default=MEMORY_LOCATIONS)\n",
        "\tparser.add_argument('--memory-word-size',\n",
        "\t\t\tdest='_memory_word_size', help='Size of each word in memory (default: %(default)s)',\n",
        "\t\t\ttype=int, default=MEMORY_WORD_SIZE)\n",
        "\tparser.add_argument('--learning-rate',\n",
        "\t\t\tdest='_learning_rate', help='Learning Rate (default: %(default)s)',\n",
        "\t\t\ttype=float, default=LEARNING_RATE)\n",
        "\tparser.add_argument('--iterations',\n",
        "\t\t\tdest='_iterations', help='Number of iterations for training (default: %(default)s)',\n",
        "\t\t\ttype=int, default=ITERATIONS)\n",
        "\n",
        "\treturn parser\n",
        "\n",
        "\n",
        "def omniglot():\n",
        "\n",
        "\t# parser = build_argparser()\n",
        "\t# args = parser.parse_args()\n",
        "\n",
        "\t# batch_size = args._batch_size\n",
        "\t# nb_classes = args._nb_classes\n",
        "\t# nb_samples = args._nb_samples\n",
        "\t# img_size = (args._input_height, args._input_width)\n",
        "\t# input_size = args._input_height * args._input_width\n",
        "\n",
        "\t# nb_reads = args._nb_reads\n",
        "\t# controller_size = args._controller_size\n",
        "\t# memory_size = (args._memory_locations, args._memory_word_size)\n",
        "\t\n",
        "\t# learning_rate = args._learning_rate\n",
        "\t# max_iter = args._iterations\n",
        "\tbatch_size = BATCH_SIZE\n",
        "\tnb_classes = NB_CLASSES\n",
        "\tnb_samples = NB_SAMPLES\n",
        "\timg_size = (INPUT_HEIGHT, INPUT_WIDTH)\n",
        "\tinput_size = INPUT_HEIGHT*INPUT_WIDTH\n",
        "\n",
        "\tnb_reads = NB_READS\n",
        "\tcontroller_size = CONTROLLER_SIZE\n",
        "\tmemory_size = (MEMORY_LOCATIONS,MEMORY_WORD_SIZE)\n",
        "\n",
        "\tlearning_rate = LEARNING_RATE\n",
        "\tmax_iter = ITERATIONS\n",
        "\n",
        "\tinput_var = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_samples, input_size))\n",
        "\ttarget_var = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_samples))\n",
        "\n",
        "\t# # choose a local (colab) directory to store the data.\n",
        "  # local_download_path = os.path.expanduser('~/data/annotation')\n",
        "  # try:\n",
        "  #   os.makedirs(local_download_path)\n",
        "  # except: \n",
        "  #   pass\n",
        "\n",
        "  # # 2. Auto-iterate using the query syntax\n",
        "  # #    https://developers.google.com/drive/v2/web/search-parameters\n",
        "  # file_list = drive.ListFile({'q': \"'1JInWt48attjYTuTyc2i0x8guCyF6KNBd' in parents\"}).GetList()\n",
        "\n",
        "  # for f in file_list:\n",
        "  #   # 3. Create & download by id.\n",
        "  #   print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  #   fname = os.path.join(local_download_path, f['title'])\n",
        "  #   print('downloading to {}'.format(fname))\n",
        "  #   f_ = drive.CreateFile({'id': f['id']})\n",
        "  #   f_.GetContentFile(fname)\n",
        "\t\n",
        "\tgenerator = OmniglotGenerator(data_folder='drive/My Drive/omniglot_data/images_background', batch_size=batch_size, nb_classes=nb_classes, \\\n",
        "\t\t\tnb_samples=nb_samples, max_rotation=0., max_shift=0, img_size=img_size)\n",
        "\n",
        "\tnet = mann(input_size=input_size, memory_size=memory_size, controller_size=controller_size, \\\n",
        "\t\t\tnb_reads=nb_reads, nb_classes=nb_classes, batch_size=batch_size)\n",
        "\toutput_var, params = net.compute_output(input_var, target_var)\n",
        "\n",
        "\ti = 0\n",
        "\twith tf.variable_scope('weights0', reuse=tf.AUTO_REUSE):\n",
        "\t\tW_key = tf.get_variable('W_key', shape=(nb_reads, controller_size, memory_size[1]))\n",
        "\t\tb_key = tf.get_variable('b_key', shape=(nb_reads, memory_size[1]))\n",
        "\n",
        "\t\tW_sigma = tf.get_variable('W_sigma', shape=(nb_reads, controller_size, 1))\n",
        "\t\tb_sigma = tf.get_variable('b_sigma', shape=(nb_reads, 1))\n",
        "\n",
        "\t\tW_xh = tf.get_variable('W_xh', shape=(input_size + nb_classes, 4*controller_size))\n",
        "\t\tW_hh = tf.get_variable('W_hh', shape=(controller_size, 4*controller_size))\n",
        "\t\tb_h = tf.get_variable('b_h', shape=(4*controller_size))\n",
        "\n",
        "\t\tW_o = tf.get_variable('W_o', shape=(controller_size + nb_reads * memory_size[1], nb_classes))\n",
        "\t\tb_o = tf.get_variable('b_o', shape=(nb_classes))\n",
        "\n",
        "\t\tgamma = 0.95\n",
        "\n",
        "\tparams = [W_key, b_key, W_sigma, b_sigma, W_xh, W_hh, b_h, W_o, b_o]\n",
        "\n",
        "\ttarget_one_hot = tf.one_hot(target_var, nb_classes, axis=-1)\n",
        "\tcost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_var, labels=target_one_hot), name=\"cost\")\n",
        "\tacc = tf.reduce_mean(tf.cast(tf.equal(target_var, tf.cast(tf.argmax(output_var, axis=2), dtype=tf.int32)), dtype=tf.float32))\n",
        "\n",
        "\topt = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.95, momentum=0.9)\n",
        "\ttrain_step = opt.minimize(cost, var_list=params)\n",
        "\n",
        "\n",
        "\tsess = tf.Session()\n",
        "\tinit = tf.global_variables_initializer()\n",
        "\tsess.run(init)\n",
        "\t\n",
        "\titers = []\n",
        "\taccuracies = []\n",
        "\tcosts = []\n",
        "\n",
        "\twith sess.as_default():\n",
        "\t\ttry:\n",
        "\t\t\tfor i in range(max_iter):\n",
        "\t\t\t\tepisode_input, episode_output = generator.episode()\n",
        "\t\t\t\tfeed_dict = {input_var: episode_input, target_var: episode_output}\n",
        "\t\t\t\ttrain_step.run(feed_dict)\n",
        "\t\t\t\tif i % 10 == 0:\t\t#(max_iter*1e-3)\n",
        "\t\t\t\t\tcost_val = sess.run(cost, feed_dict=feed_dict)\n",
        "\t\t\t\t\tacc_val = sess.run(acc, feed_dict=feed_dict)\n",
        "\t\t\t\t\titers.append(i)\n",
        "\t\t\t\t\tcosts.append(cost_val)\n",
        "\t\t\t\t\taccuracies.append(acc_val)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tprint('Target Labels:')\n",
        "\t\t\t\t\tprint(sess.run(target_var[0], feed_dict=feed_dict))\n",
        "\t\t\t\t\tprint('Model Output:')\n",
        "\t\t\t\t\tprint(sess.run(tf.argmax(output_var[0], axis=1), feed_dict=feed_dict))\n",
        "\t\t\t\t\tprint('Episode ' + str(i) + ': Cost = ' + str(cost_val) + '\\t Accuracy = ' + str(acc_val))\n",
        "\t\t\t\t\tprint('')\n",
        "\n",
        "\t\t\t\t\twith open('omniglot-cost', 'wb') as fp:\n",
        "\t\t\t\t\t\tpickle.dump(costs, fp)\n",
        "\n",
        "\t\t\t\t\twith open('omniglot-acc', 'wb') as fp:\n",
        "\t\t\t\t\t\tpickle.dump(accuracies, fp)\n",
        "\n",
        "\t\t\t\t\twith open('omniglot-iters', 'wb') as fp:\n",
        "\t\t\t\t\t\tpickle.dump(iters, fp)\n",
        "\n",
        "\t\texcept KeyboardInterrupt:\n",
        "\t\t\tprint('\\nInterrupted at Episode ' + str(i))\n",
        "\t\t\tprint('Cost = ' + str(cost_val))\n",
        "\t\t\tprint('Accuracy = ' + str(acc_val))\n",
        "\t\t\tpass\n",
        "\n",
        "\t\n",
        "\tfig = plt.figure(figsize=(20,8))\n",
        "\tplt.subplot(1,2,1)\n",
        "\tplt.plot(iters, costs, 'b', label='Training Error', linewidth=2, alpha=0.8)\n",
        "\tplt.xlabel('Episodes', fontsize=22)\n",
        "\tplt.ylabel('Cross Entropy Loss', fontsize=22)\n",
        "\tplt.title('Training Error', fontsize=26)\n",
        "\n",
        "\tplt.subplot(1,2,2)\n",
        "\tplt.plot(iters, accuracies, 'b-', label='Training Accuracy', linewidth=2, alpha=0.8)\n",
        "\tplt.xlabel('Episodes', fontsize=22)\n",
        "\tplt.ylabel('Accuracy', fontsize=22)\n",
        "\tplt.title('Training Accuracy', fontsize=26)\n",
        "\tplt.show()\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "omniglot()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KsPC16bSJRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}