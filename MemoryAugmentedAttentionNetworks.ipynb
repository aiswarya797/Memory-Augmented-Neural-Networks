{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemoryAugmentedAttentionNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JN_2mOteG43OHETZDo87QgUnG7u4llZu",
      "authorship_tag": "ABX9TyMXAhftvmlklKgKFbehEY2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiswarya797/Memory-Augmented-Neural-Networks/blob/master/MemoryAugmentedAttentionNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTxJJPYBCMVx"
      },
      "source": [
        "* Check the data input and output \n",
        "  * what is the embedding which needs to be used?\n",
        "  * The output sizes can be different\n",
        "  * In what way data has to be inputted?\n",
        "* What is the loss function to be used?\n",
        "* Is the memory to attention?\n",
        "\n",
        "* References:\n",
        "\n",
        "  -- https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb\n",
        "  \n",
        "  -- https://www.guru99.com/seq2seq-model.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ZwX30oZUB6jA",
        "outputId": "02582263-b0f9-4924-e1d2-23bb57d50fef"
      },
      "source": [
        "## Data Set Loader\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def Convert(string): \n",
        "    list1=[] \n",
        "    list1[:0]=string \n",
        "    return list1 \n",
        "\n",
        "def generate_seq(filename, write_in_file=False):\n",
        "  character_sequence = []\n",
        "  subword_sequence = []\n",
        "\n",
        "  if(write_in_file):\n",
        "    f = open(\"/media/aiswarya/New Volume/My_works/MANN/Model0/Data/SeqData.txt\",\"w\")\n",
        "    \n",
        "  file_ = open(filename, 'r') \n",
        "  lines = file_.readlines() \n",
        "  for line in lines:\n",
        "    temp = line.split(':')\n",
        "    if len(temp)<=2:\n",
        "      t = line.split(',')\n",
        "      temp = t[0].split(':')\n",
        "    char_seq = temp[0].strip()\n",
        "    subwrd_seq = temp[1].strip()\n",
        "    subword_sequence.append(subwrd_seq)\n",
        "    character_sequence.append(char_seq)\n",
        "    if(write_in_file):\n",
        "        f.write(\"%s\\t%s\\n\"%(char_seq,subwrd_seq))\n",
        "  if(write_in_file):\n",
        "    f.close()\n",
        "  return character_sequence,subword_sequence\n",
        "\n",
        "def load_data(data_file):\n",
        "    f = open(data_file)\n",
        "    line = f.readline()\n",
        "    X = []\n",
        "    Y = []\n",
        "    while line:\n",
        "        info = line.strip(\"\\n\").split(\"\\t\")\n",
        "        x = info[0]\n",
        "        y = info[1]\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "        line = f.readline()\n",
        "    f.close()\n",
        "    return X,Y\n",
        "\n",
        "def train_test_split(X,Y, split_rat = 0.8):\n",
        "  length = len(X)\n",
        "  train_size = int(length*0.8)\n",
        "  test_size = length-train_size\n",
        "  X_train = X[0:train_size]\n",
        "  Y_train = Y[0:train_size]\n",
        "  X_test = X[train_size:]\n",
        "  Y_test = Y[train_size:]\n",
        "\n",
        "  return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def find_distinct_tokens(data,separator, write_in_file, write = False):\n",
        "  tok2ind = {}\n",
        "  ind2tok = {}\n",
        "  all_tok = []\n",
        "  for d in data:\n",
        "    info = d.split(separator)\n",
        "    all_tok.extend(info)\n",
        "    \n",
        "  all_tok = list(set(all_tok))\n",
        "  all_tok.sort()\n",
        "  all_tok.extend([\"<START>\",\"<END>\"])\n",
        "  print(\"all_tok\")\n",
        "  if write:\n",
        "    f = open(write_in_file,\"w\")\n",
        "    for ind,item in enumerate(all_tok):\n",
        "      tok2ind[item] = ind\n",
        "      ind2tok[ind] = item\n",
        "      f.write(\"%s\\t%d\\n\"%(item,ind))\n",
        "    f.close()\n",
        "  else:\n",
        "    for ind,item in enumerate(all_tok):\n",
        "      tok2ind[item] = ind\n",
        "      ind2tok[ind] = item\n",
        "  return tok2ind,ind2tok\n",
        "\n",
        "def load_tokens(token_file):\n",
        "    f = open(token_file)\n",
        "    line = f.readline()\n",
        "    tok2ind = {}\n",
        "    ind2tok = {}\n",
        "    while line:\n",
        "        info = line.strip(\"\\n\").split(\"\\t\")\n",
        "        item = info[0]\n",
        "        ind = int(info[1])\n",
        "        tok2ind[item] = ind\n",
        "        ind2tok[ind] = item\n",
        "        line = f.readline()\n",
        "    return tok2ind,ind2tok\n",
        "\n",
        "def string2index(datestring,tok2ind,separator):\n",
        "    components = datestring.split(separator)\n",
        "    return [tok2ind[i] for i in components]\n",
        "\n",
        "def batch_generator(x_data,y_data,batch_size,source_tok2ind,target_tok2ind):\n",
        "    total = len(x_data)\n",
        "    print('total : ', total)\n",
        "    start = 0\n",
        "    # i = 0\n",
        "    while True:\n",
        "        end = min(start + batch_size , total)\n",
        "        X = x_data[start:end]\n",
        "        Y = y_data[start:end]\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for bx,by in zip(X,Y):\n",
        "            bx = string2index(bx,source_tok2ind,\"-\")\n",
        "            by = string2index(by,target_tok2ind,\",\")\n",
        "            batch_x.append(bx)\n",
        "            batch_y.append(by)\n",
        "        yield np.asarray(batch_x),np.asarray(batch_y),start,end\n",
        "        # print(np.asarray(batch_x), '  ',np.asarray(batch_y),'  ', start, '  ', end)\n",
        "        start = end\n",
        "        if(start >= total):\n",
        "            start = 0\n",
        "            \n",
        "        # i+=1\n",
        "    # return np.asarray(batch_x), np.asarray(batch_y)\n",
        "\n",
        "## Test ##\n",
        "\"\"\"\n",
        "X,Y = generate_seq(\"drive/My Drive/AttentionModel/datafile.txt\", False)\n",
        "print(len(X))\n",
        "print(len(Y))\n",
        "\n",
        "print(X[160], '      ', Y[160])\n",
        "print(X[1010], '     ', Y[1010])\n",
        "print(X[10], '      ', Y[10])\n",
        "print(X[234], '     ', Y[234])\n",
        "print(X[513], '      ', Y[513])\n",
        "print(X[798], '     ', Y[798])\n",
        "\n",
        "ST, SI = find_distinct_tokens(X, '-', \"\", False)\n",
        "TT,TI = find_distinct_tokens(Y, ',', \"\", False)\n",
        "bx, by = batch_generator(X,Y,5,ST,TT)\n",
        "bx = np.array(bx).flatten().tolist()\n",
        "by = np.array(by).flatten().tolist()\n",
        "\n",
        "for i in range(len(bx)):\n",
        "  print(bx[i])\n",
        "  print(SI[bx[i]])\n",
        "\n",
        "for i in range(len(by)):\n",
        "  print(by[i])\n",
        "  for j in range(len(by[i])):\n",
        "    y = by[i][j]\n",
        "    print(TI[y])\n",
        "\"\"\"\n",
        "### OUTPUT ###\n",
        "\"\"\"\n",
        "188749\n",
        "188749\n",
        "शही        शही\n",
        "गधा       ग,धा\n",
        "पौढ़        पौ,ढ़\n",
        "थ्र       थ्र\n",
        "जाय        जाय\n",
        "हौं       हौ,ं\n",
        "all_tok\n",
        "all_tok\n",
        "total :  188749\n",
        "109714\n",
        "बस\n",
        "184335\n",
        "०\n",
        "39670\n",
        "खचा\n",
        "145053\n",
        "लिया।\n",
        "49826\n",
        "चकी\n",
        "[3617]\n",
        "बस\n",
        "[6064]\n",
        "०\n",
        "[1325, 1675]\n",
        "ख\n",
        "चा\n",
        "[4651, 5938]\n",
        "लिय\n",
        "ा\n",
        "[1634, 1133]\n",
        "च\n",
        "की\n",
        "\n",
        "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
        "  return array(a, dtype, copy=False, order=order)\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n188749\\n188749\\nशही        शही\\nगधा       ग,धा\\nपौढ़        पौ,ढ़\\nथ्र       थ्र\\nजाय        जाय\\nहौं       हौ,ं\\nall_tok\\nall_tok\\ntotal :  188749\\n109714\\nबस\\n184335\\n०\\n39670\\nखचा\\n145053\\nलिया।\\n49826\\nचकी\\n[3617]\\nबस\\n[6064]\\n०\\n[1325, 1675]\\nख\\nचा\\n[4651, 5938]\\nलिय\\nा\\n[1634, 1133]\\nच\\nकी\\n\\n/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\\n  return array(a, dtype, copy=False, order=order)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AeI7suAFDK0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "REDUCE = 1\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "if(physical_devices):\n",
        "    tf.config.experimental.set_virtual_device_configuration(physical_devices[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4xpJb6AO7gf"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input,LSTM,Reshape,Conv2D,Flatten, GRU"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcabQ5iePDRK"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, enc_units, vocab_size, embedding_dim=32, nb_layers=3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.nb_layers = nb_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.all_layers = []\n",
        "        for l in range(self.nb_layers):\n",
        "            layer = tf.keras.layers.GRU(self.enc_units, return_sequences=True,\n",
        "                                        return_state=True, recurrent_initializer='glorot_uniform', dropout=0.2)\n",
        "            self.all_layers.append(layer)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        x = self.embedding(inputs)\n",
        "        for l in range(len(self.all_layers)):\n",
        "            output, state = self.all_layers[l](x)\n",
        "            x = output\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.enc_units))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kEXMrqSPFoH"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh6AvAwEPOSW"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units,memory_size= 20,memory_vector_dim= 4,head_num= 2, nb_layers=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.all_layers = []\n",
        "        for i in range(self.nb_layers):\n",
        "          layer = GRU(units=128,return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', dropout=0.2)\n",
        "          self.all_layers.append(layer)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_vector_dim = memory_vector_dim\n",
        "        self.head_num = head_num\n",
        "        self.gamma = 0.95\n",
        "        self.reuse = True\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "  \n",
        "    def read_head_addressing(self, k, prev_M):    \n",
        "\n",
        "        # print('k shape : ', k.shape)\n",
        "        k = tf.expand_dims(k, axis=3)\n",
        "        # print('expand k shape : ', k.shape)\n",
        "        # print('M shape : ', prev_M.shape)\n",
        "        inner_product = tf.matmul(prev_M, k)\n",
        "        # print('inner pd shape : ', inner_product.shape)\n",
        "        k_norm = tf.sqrt(tf.reduce_sum(tf.square(k), axis=2, keepdims=True))\n",
        "        # print('k norm shape : ', k_norm.shape)\n",
        "        M_norm = tf.sqrt(tf.reduce_sum(tf.square(prev_M), axis=3, keepdims=True))\n",
        "        # print('M norm shape : ', M_norm.shape)\n",
        "        norm_product = M_norm * k_norm\n",
        "        # print('norm pd shape : ', norm_product.shape)\n",
        "        K = tf.expand_dims(tf.expand_dims(tf.squeeze(inner_product / (norm_product + 1e-8)), axis = 0), axis=0)\n",
        "        # print('K squeeze shape : ', K.shape)                \n",
        "        K_exp = tf.exp(K)\n",
        "        w = K_exp / tf.reduce_sum(K_exp, axis=1, keepdims=True)    \n",
        "        # print('w shape : ', w.shape)                   \n",
        "        \n",
        "        return w\n",
        "\n",
        "    #weight vector for write operation\n",
        "    def write_head_addressing(self,sig_alpha, prev_w_r_list, prev_w_lu):\n",
        "        prev_w_r = prev_w_r_list[-1]\n",
        "        return sig_alpha * prev_w_r + (1. - sig_alpha) * prev_w_lu     \n",
        "\n",
        "    #least used weight vector\n",
        "    def least_used(self,w_u):\n",
        "        _, indices = tf.nn.top_k(w_u, k=self.memory_size)\n",
        "        w_lu = tf.reduce_sum(tf.one_hot(indices[:, -self.head_num:], depth=self.memory_size), axis=2)\n",
        "        return indices, w_lu\n",
        "\n",
        "\n",
        "    #next we define the function called zero state for initializing all the states - \n",
        "    #controller state, read vector, weights and memory\n",
        "    def zero_state(self,batch_size,dtype):\n",
        "        one_hot_weight_vector = np.zeros([batch_size,1, self.memory_size])\n",
        "        one_hot_weight_vector[..., 0] = 1\n",
        "        one_hot_weight_vector = tf.constant(one_hot_weight_vector, dtype=tf.float32)\n",
        "        with tf.compat.v1.variable_scope('init', reuse=self.reuse):\n",
        "            state = {\n",
        "                'read_vector_list': [tf.zeros([batch_size,1, self.memory_vector_dim])\n",
        "                                      for _ in range(self.head_num)],\n",
        "                'w_r_list': [one_hot_weight_vector for _ in range(self.head_num)],\n",
        "                'w_u': one_hot_weight_vector,\n",
        "                'M': tf.constant(np.ones([batch_size,1, self.memory_size, self.memory_vector_dim]) * 1e-6, dtype=tf.float32)\n",
        "            }\n",
        "            return state\n",
        "\n",
        "    def call(self, x, hidden, enc_output, prev_state):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output) #(batch_size, hidden_size)\n",
        "        prev_read_vector_list = prev_state['read_vector_list']\n",
        "        context_vector = tf.reshape(context_vector, (context_vector.shape[0], 1, context_vector.shape[1]))\n",
        "        context_vector = tf.concat([context_vector] + prev_read_vector_list, axis=-1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        # print('embedded x shape : ', x.shape)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size +prev_read_size)\n",
        "        x = tf.concat([context_vector, x], axis=-1)\n",
        "        # print('final x shape : ', x.shape)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        layer_output = x\n",
        "        for l in range(len(self.all_layers)):\n",
        "\n",
        "            output, state = self.all_layers[l](layer_output)\n",
        "            layer_output = output\n",
        "\n",
        "        network = Sequential()\n",
        "        network.add(LSTM(units=128,activation='tanh',return_sequences=True))\n",
        "        layer_output = network(layer_output)\n",
        "\n",
        "        \n",
        "        num_parameters_per_head = self.memory_vector_dim + 1\n",
        "        total_parameter_num = num_parameters_per_head * self.head_num\n",
        "        \n",
        "        #Initiliaze weight matrix and bias and compute the parameters\n",
        "        weights = tf.Variable(tf.random.normal([layer_output.get_shape()[0],layer_output.get_shape()[2], total_parameter_num], stddev=0.35))\n",
        "        # print('weights para : ', weights.shape)\n",
        "        biases = tf.Variable(tf.zeros([total_parameter_num]))\n",
        "        parameters = tf.compat.v1.nn.xw_plus_b(layer_output, weights, biases)\n",
        "        # print('parameters : ', parameters.shape)\n",
        "        head_parameter_list = tf.split(parameters, self.head_num, axis=2)\n",
        "        # print('head parameter list : ', np.array(head_parameter_list).shape)\n",
        "        \n",
        "        \n",
        "        #previous read weight vector\n",
        "        prev_w_r_list = prev_state['w_r_list']   \n",
        "        # print('w_r weights : ', prev_w_r_list[1].shape)\n",
        "        \n",
        "        #previous memory\n",
        "        prev_M = prev_state['M']\n",
        "        # print('memory : ', prev_M.shape)\n",
        "        \n",
        "        #previous usage weight vector\n",
        "        prev_w_u = prev_state['w_u']\n",
        "        # print('usage weights : ', prev_w_u.shape)\n",
        "        \n",
        "        #previous index and least used weight vector\n",
        "        prev_indices, prev_w_lu = self.least_used(prev_w_u)\n",
        "        \n",
        "        #read weight vector\n",
        "        w_r_list = []\n",
        "        \n",
        "        #write weight vector\n",
        "        w_w_list = []\n",
        "        \n",
        "        #key vector\n",
        "        k_list = []\n",
        "    \n",
        "        #now, we will initialize some of the important parameters that we use for addressing.     \n",
        "        for i, head_parameter in enumerate(head_parameter_list):\n",
        "            with tf.compat.v1.variable_scope('addressing_head_%d' % i):\n",
        "                \n",
        "                #key vector\n",
        "                k = tf.tanh(head_parameter[:,:, 0:self.memory_vector_dim], name='k')\n",
        "\n",
        "                #sig_alpha\n",
        "                sig_alpha = tf.sigmoid(head_parameter[:,:, -1:], name='sig_alpha')\n",
        "                \n",
        "                #read weights\n",
        "                w_r = self.read_head_addressing(k, prev_M)\n",
        "                \n",
        "                #write weights\n",
        "                w_w = self.write_head_addressing(sig_alpha, prev_w_r_list[i], prev_w_lu)\n",
        "           \n",
        "            w_r_list.append(w_r)\n",
        "            w_w_list.append(w_w)\n",
        "            k_list.append(k)\n",
        "            \n",
        "        # print('new w_R : ', w_r_list[0].shape)\n",
        "        #usage weight vector \n",
        "        w_u = self.gamma * prev_w_u + tf.add_n(w_r_list) + tf.add_n(w_w_list)   \n",
        "\n",
        "        #update the memory\n",
        "        M_ = prev_M * tf.compat.v1.expand_dims(1. - tf.one_hot(prev_indices[:,:, -1], self.memory_size), dim=3)\n",
        "        \n",
        "        #write operation\n",
        "        M = M_\n",
        "        with tf.compat.v1.variable_scope('writing'):\n",
        "            for i in range(self.head_num):\n",
        "                \n",
        "                w = tf.expand_dims(w_w_list[i], axis=3)\n",
        "                k = tf.expand_dims(k_list[i], axis=2)\n",
        "                M = M + tf.matmul(w, k)\n",
        "\n",
        "        #read opearion\n",
        "        read_vector_list = []\n",
        "        with tf.compat.v1.variable_scope('reading'):\n",
        "            for i in range(self.head_num):\n",
        "                read_vector = tf.reduce_sum(tf.compat.v1.expand_dims(w_r_list[i], dim=3) * M, axis=2)\n",
        "                read_vector_list.append(read_vector)       \n",
        "\n",
        "        \n",
        "        #controller output\n",
        "        new_state = {\n",
        "            'read_vector_list': read_vector_list,\n",
        "            'w_r_list': w_r_list,\n",
        "            'w_w_list': w_w_list,\n",
        "            'w_u': w_u,\n",
        "            'M': M,\n",
        "        }\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights, new_state"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxTpVrNz6QwT"
      },
      "source": [
        "class Seq2SeqMANN:\n",
        "    def __init__(self, name, in_vocab_size, out_vocab_size, embedding_dim, units, batch_size):\n",
        "        self.name = name\n",
        "        self.units = units\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder = Encoder(units, in_vocab_size)\n",
        "        print(\"Encoder Dimensions \", self.encoder)\n",
        "        self.decoder = Decoder(out_vocab_size, embedding_dim, units)\n",
        "        self.decoder_state = self.decoder.zero_state(self.batch_size, tf.float32)\n",
        "        print(\"Decoder Dimensions \", self.decoder)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(\n",
        "            0.0001)  # tf.keras.optimizers.RMSprop(0.0001)\n",
        "        self.saver = tf.train.Checkpoint(\n",
        "            encoder=self.encoder, decoder=self.decoder, optimizer=self.optimizer)\n",
        "        self.save_dir = \"drive/My Drive/AttentionModel/Weights/\"+self.name+\".ckpt\"\n",
        "        self.weight_manager = tf.train.CheckpointManager(\n",
        "            self.saver, self.save_dir, max_to_keep=2)\n",
        "        self.loss_list = []\n",
        "\n",
        "    def loss_function(self, pred, real):\n",
        "\n",
        "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction='none')\n",
        "\n",
        "        loss_ = loss_object(real, pred)\n",
        "        return tf.reduce_mean(loss_)\n",
        "\n",
        "    def train_step(self, inp, targ, start_index, end_index):\n",
        "        loss = 0\n",
        "        batch_size = inp.shape[0]\n",
        "\n",
        "        pred_output = []\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = self.encoder(inp)\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_input = tf.expand_dims([start_index] * batch_size,1)  #start index = index of <START>\n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            # print('targ :', targ.shape)\n",
        "            # print('targ,shape[1] : ', targ.shape[1])\n",
        "            for t in range(0, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                prediction, dec_hidden, att_wt, state = self.decoder(dec_input, dec_hidden, enc_output, self.decoder_state)\n",
        "                loss += self.loss_function(prediction, targ[:, t])\n",
        "                self.decoder_state = state\n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1) #tf.math.argmax(prediction), 1)\n",
        "                # if(target[t] == end_index):\n",
        "                #   break\n",
        "        \n",
        "        # batch_loss = (loss / int(targ.shape[1]))\n",
        "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        self.loss_list.append(loss) \n",
        "        return loss\n",
        "\n",
        "    def gnrl(self, inp, targ, start_index, max_length, target_ind2tok):\n",
        "        enc_output, enc_hidden = self.encoder(inp)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([start_index] * self.batch_size,1)  #start index = index of <START>\n",
        "\n",
        "        result = ''\n",
        "        for t in range(max_length):\n",
        "            prediction, dec_hidden, attention_weights, state = self.decoder(dec_input, dec_hidden, enc_output, self.decoder_state)\n",
        "            # storing the attention weights to plot later on\n",
        "            attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "            # attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "            predicted_id = tf.math.argmax(input = prediction).numpy()\n",
        "            \n",
        "            max = -1\n",
        "            for i in predicted_id:\n",
        "              if(max<i):\n",
        "                max = i\n",
        "\n",
        "            predicted_id = max\n",
        "            print('Predicted ID : ', predicted_id)\n",
        "            result += target_ind2tok[predicted_id] + ' '\n",
        "\n",
        "            if target_ind2tok[predicted_id] == '<END>':\n",
        "                return result, targ#, attention_plot\n",
        "\n",
        "            # the predicted ID is fed back into the model\n",
        "            dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        return result, targ#, attention_plot\n",
        "\n",
        "\n",
        "    def train(self, X, Y, source_tok2ind, target_ind2tok, target_tok2ind, epochs):\n",
        "        restore_from = tf.train.latest_checkpoint(self.save_dir)\n",
        "        self.saver.restore(restore_from)\n",
        "        print('Weights restored from %s' % restore_from)\n",
        "\n",
        "        total = len(X)\n",
        "        batches = batch_generator(X, Y, self.batch_size,\n",
        "                                    source_tok2ind, target_tok2ind)\n",
        "        nbbatches = int(np.ceil(total / float(self.batch_size)))\n",
        "        print(\"Ready to generate batches: Total %d #Batches %d\" %\n",
        "                (total, nbbatches))\n",
        "        best_loss = np.inf\n",
        "        print('epochs', epochs)\n",
        "        i = 1\n",
        "        print('Iteration : ', i)\n",
        "        for e in range(epochs):\n",
        "            total_loss = 0\n",
        "            for b in range(nbbatches):\n",
        "                batch_x, batch_y, start, end = next(batches)\n",
        "                if batch_x.shape[0] == self.batch_size:\n",
        "                  b_loss = self.train_step(\n",
        "                      batch_x, batch_y, start_index=target_tok2ind['<START>'], end_index=target_tok2ind['<END>'])\n",
        "                  total_loss += b_loss\n",
        "                i+=1\n",
        "                if i%50==0:\n",
        "                  t = np.arange(0,i-1,1)\n",
        "                  plt.plot(self.loss_list, t)\n",
        "                  plt.show()\n",
        "                if i%10==0:\n",
        "                    self.predict(batch_x, batch_y, target_tok2ind['<START>'], 5,source_ind2tok, target_ind2tok)\n",
        "                    print('Iteration : ', i)\n",
        "                    print('Loss : ', b_loss)\n",
        "            total_loss = total_loss/nbbatches\n",
        "            print(\"\\tEpoch %d/%d Loss %0.4f\" % (e+1, epochs, total_loss))\n",
        "            if(total_loss < best_loss):\n",
        "                self.weight_manager.save()\n",
        "                best_loss = total_loss\n",
        "                print(\"--------------New Best State-----------------\")\n",
        "\n",
        "    def predict(self, inp, targ, start_index, max_length, source_ind2tok, target_ind2tok):\n",
        "\n",
        "        # print(inp)\n",
        "        result, targ = self.gnrl(inp, targ, start_index, max_length, target_ind2tok)\n",
        "        # print('Input: ', source_ind2tok[inp[0][0]])\n",
        "        print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "480hfdFpJBed"
      },
      "source": [
        "X, Y = generate_seq(\"drive/My Drive/AttentionModel/datafile.txt\", False)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)\n",
        "\n",
        "source_tok2ind, source_ind2tok = find_distinct_tokens(X, '-', write_in_file=\"/media/aiswarya/New Volume/My_works/MANN/Model0/Data/source_tokens.txt\", write=False)\n",
        "target_tok2ind, target_ind2tok = find_distinct_tokens(Y, ',',  write_in_file=\"/media/aiswarya/New Volume/My_works/MANN/Model0/Data/target_tokens.txt\", write=False)\n",
        "\n",
        "print(source_ind2tok[0])\n",
        "\n",
        "Nc_out = len(target_tok2ind)\n",
        "print('Nc Out : ', Nc_out)\n",
        "Nc_in = len(source_tok2ind)\n",
        "print('Nc In : ', Nc_in)\n",
        "\n",
        "network = Seq2SeqMANN('MANN', Nc_in, Nc_out, embedding_dim=32, units=128, batch_size=1)\n",
        "\n",
        "# train network \n",
        "network.train(X_train, Y_train, source_tok2ind, target_ind2tok, target_tok2ind, 50)\n",
        "# network.test(X_tes"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}